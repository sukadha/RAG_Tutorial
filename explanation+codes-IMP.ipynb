{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5ff887f",
   "metadata": {},
   "source": [
    "This loads environment variables from a .env file into your system environment so you can securely access them in your code using os.getenv()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf1be6b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa1a822",
   "metadata": {},
   "source": [
    "This checks whether the OPENAI_API_KEY exists in the environment variables and prints a message if it is set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99be1f46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API Key is set.\n"
     ]
    }
   ],
   "source": [
    "if os.environ['OPENAI_API_KEY']:\n",
    "    print(\"API Key is set.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af5efbc",
   "metadata": {},
   "source": [
    "This imports the ChatOpenAI class from LangChain’s OpenAI integration, which allows you to interact with OpenAI chat models (like GPT) inside your Python application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69d9dea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e96909",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\",temperature=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33410e26",
   "metadata": {},
   "source": [
    "This creates a ChatOpenAI language model instance using the \"gpt-5-nano\" model with temperature set to 0, meaning the responses will be more deterministic and consistent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00e6099a",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-5-nano\",temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ed96dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm.invoke(\"What is AI? Tell me in one line\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7413a3",
   "metadata": {},
   "source": [
    "This sends the prompt to the language model and stores the response in result; result.content extracts and displays the generated text from the model’s reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "860f586c",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = llm.invoke(\"What is AI? Tell me in one line\")\n",
    "result.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ced11b",
   "metadata": {},
   "source": [
    "### RAG Implementation With your own text data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e92358c6",
   "metadata": {},
   "source": [
    "### Step 1 : Preparing Document for your Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d01b0a6",
   "metadata": {},
   "source": [
    "This imports the Document class from LangChain, which is used to represent text data along with optional metadata for processing in LLM or RAG workflows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc85b375",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128041c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_text = \"\"\"Artificial intelligence (AI) is the capability of computational systems to perform tasks typically associated with human intelligence, such as learning, reasoning, problem-solving, perception, and decision-making. It is a field of research in computer science that develops and studies methods and software that enable machines to perceive their environment and use learning and intelligence to take actions that maximize their chances of achieving defined goals.[1]\n",
    "\n",
    "High-profile applications of AI include advanced web search engines (e.g., Google Search); recommendation systems (used by YouTube, Amazon, and Netflix); virtual assistants (e.g., Google Assistant, Siri, and Alexa); autonomous vehicles (e.g., Waymo); generative and creative tools (e.g., language models and AI art); and superhuman play and analysis in strategy games (e.g., chess and Go). However, many AI applications are not perceived as AI: \"A lot of cutting edge AI has filtered into general applications, often without being called AI because once something becomes useful enough and common enough it's not labeled AI anymore.\"[2][3]\n",
    "\n",
    "Various subfields of AI research are centered around particular goals and the use of particular tools. The traditional goals of AI research include learning, reasoning, knowledge representation, planning, natural language processing, perception, and support for robotics.[a] To reach these goals, AI researchers have adapted and integrated a wide range of techniques, including search and mathematical optimization, formal logic, artificial neural networks, and methods based on statistics, operations research, and economics.[b] AI also draws upon psychology, linguistics, philosophy, neuroscience, and other fields.[4] Some companies, such as OpenAI, Google DeepMind and Meta,[5] aim to create artificial general intelligence (AGI) – AI that can complete virtually any cognitive task at least as well as a human.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7218ab1f",
   "metadata": {},
   "source": [
    "This creates a LangChain Document object where page_content stores your AI text, and wraps it inside a list (docs) so it can be used in RAG pipelines like text splitting, embedding, and retrieval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3430644",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Langchain creates a document for the above my_text \n",
    "\n",
    "docs = [Document(page_content=my_text)]\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99eadbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "### or\n",
    "\n",
    "docs = [Document(page_content=my_text, metadata={\"source\": \"ABC\",\"documnetID\":\"doc1\"})]\n",
    "docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d5c721d",
   "metadata": {},
   "source": [
    "### Step 2 : Splitting the documents into chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "082a7080",
   "metadata": {},
   "source": [
    "This splits your large document into smaller chunks of 500 characters each with 50-character overlap, making it easier for embedding and retrieval, while automatically preserving metadata for each chunk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa00f1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## It divides the document into multiple chunks and adds metadata to each chunk\n",
    "\n",
    "from langchain_core.text_splitter import RecursiveCharacterTextSplitter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942e3104",
   "metadata": {},
   "outputs": [],
   "source": [
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "chunks = splitter.split_documents(docs)\n",
    "chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de3c9c7",
   "metadata": {},
   "source": [
    "### Step 3 : Creating Embeddings for the chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71515b4a",
   "metadata": {},
   "source": [
    "This loads the OpenAI embedding model and converts the query “What is AI?” into a numerical vector representation, which can be used for similarity search in RAG systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22ecb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de1b282e",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e66f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model.embed_query(\"What is AI?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c1f0d",
   "metadata": {},
   "source": [
    "### Step 4 : Create and store embeddings in vector store"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ff9431",
   "metadata": {},
   "source": [
    "This stores your document chunks inside a Chroma vector database, automatically converting them into embeddings for similarity search.\n",
    "The loop manually generates embedding vectors for each chunk and stores them in a list, but Chroma.from_documents() already handles this internally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea08c896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db29fbc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_documents(documents=chunks,embedding=embedding_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a867ea14",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = []\n",
    "for doc in chunks:\n",
    "    vector = embedding_model.embed_documents([doc.page_content])\n",
    "    vectors.append(vector)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b8f2cc",
   "metadata": {},
   "source": [
    "### Step 5 : Sematic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a6fded",
   "metadata": {},
   "source": [
    "This searches the vector database for the top 3 most similar document chunks related to the query “What is AI” and returns the most relevant text sections based on embedding similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a23e10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore.similarity_search(\"What is AI\",k=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9f80e8",
   "metadata": {},
   "source": [
    "### Talk to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac0f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = vectorstore.similarity_search(\"What is AI\",k=3)\n",
    "llm.invoke(\"What is AI? Tell me in one line\",context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44f1afe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#or\n",
    "context = vectorstore.similarity_search(\"What is AI\",k=3)\n",
    "response = llm.invoke(f\"What is AI? Tell me in one line\",context=context)\n",
    "response.content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
